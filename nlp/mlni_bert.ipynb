{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import BertTokenizer, BertTokenizerFast, BertModel\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open reference docs\n",
    "import webbrowser as wb\n",
    "ref_list = [\n",
    "    \"https://huggingface.co/docs/transformers/model_doc/bert\",\n",
    "    \"https://huggingface.co/docs/transformers/v4.24.0/en/model_doc/bert#transformers.BertConfig\",\n",
    "    \"https://huggingface.co/docs/transformers/v4.24.0/en/model_doc/bert#transformers.BertTokenizer\",\n",
    "    \"https://arxiv.org/pdf/1810.04805.pdf\"\n",
    "]\n",
    "for url in ref_list:\n",
    "    wb.open(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "def subdict(d, keys):\n",
    "    return {key:d[key] for key in keys}\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "class MNLIDataset():\n",
    "    def __init__(self, root, type_map, tokenizer, batch_size, embedding_size):\n",
    "        self.idx = 0\n",
    "        self.type_map = type_map\n",
    "        self.bs = batch_size\n",
    "        self.es = embedding_size\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        print(\"Loading data...\")\n",
    "        with open(root, 'r') as f:\n",
    "            self.df = pd.read_json(f, lines=True)\n",
    "        self.df = self.df[['sentence1', 'sentence2', 'gold_label']]\n",
    "       \n",
    "        target_gold_label_cats = set(['neutral', 'entailment', 'contradiction'])\n",
    "        self.df = self.df.loc[self.df['gold_label'].isin(target_gold_label_cats)]\n",
    "        self.df['gold_label'] = self.df['gold_label'].apply(lambda x: self.type_mapper(x))\n",
    "        \n",
    "        print(\"Tokenizing data...\")\n",
    "        # Generate list of embeddings in format: [CLS] * [tokens of sent1] * [SEP] * [tokens of sent2] * [SEP]\n",
    "        # Returns list of dicts of tensors 'input_ids', 'attention_mask', and 'token_type_ids'\n",
    "        self.data = [self.tokenizer(self.df['sentence1'].iloc[i*self.bs:(i+1)*self.bs].tolist(), \n",
    "                                   self.df['sentence2'].iloc[i*self.bs:(i+1)*self.bs].tolist(), \n",
    "                                   return_tensors='pt',\n",
    "                                   padding=True,\n",
    "                                   truncation=True\n",
    "                                   ) for i in range(self.df.shape[0] // self.bs) ]\n",
    "        self.labels = torch.tensor(list(self.df['gold_label']))\n",
    "        del self.df\n",
    "        print(\"Done.\")\n",
    "\n",
    "    def get_batch_num(self):\n",
    "        example_num = len(self)\n",
    "        batch_num = example_num // self.bs + 1 if example_num % self.bs > 0 else example_num // self.bs\n",
    "        return batch_num\n",
    "\n",
    "    # transform class name into integer\n",
    "    def type_mapper(self, text):\n",
    "        return self.type_map[text]    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) * self.bs\n",
    "    \n",
    "    def set_idx(self, idx):\n",
    "        self.idx = idx\n",
    "\n",
    "    def get_batch(self, device):\n",
    "        # cycle through batches and restart if idx is too large\n",
    "        if self.idx >= len(self.data): \n",
    "            self.idx = 0\n",
    "        data = {k:torch.tensor(self.data[self.idx][k], device=device) for k in self.data[self.idx]}\n",
    "        batch_input = subdict(data, ['input_ids', 'attention_mask', 'token_type_ids'])\n",
    "        batch_labels = torch.tensor(self.labels[self.idx*self.bs : (self.idx+1)*self.bs], device=device)\n",
    "        self.idx += 1\n",
    "        return batch_input, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Tokenizing data...\n",
      "Done.\n",
      "Loading data...\n",
      "Tokenizing data...\n",
      "Done.\n",
      "Loading data...\n",
      "Tokenizing data...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "bs = 8 # about the best I can do with 8 Gb VRAM and no frozen layers\n",
    "debug_data = MNLIDataset(root='data/multinli_1.0_train_debug.json', \n",
    "                         type_map={'neutral':0, 'entailment':1, 'contradiction':2},\n",
    "                         tokenizer=BertTokenizerFast.from_pretrained('bert-base-uncased'),\n",
    "                         batch_size=bs,\n",
    "                         embedding_size=512\n",
    "                         )\n",
    "\n",
    "train_data = MNLIDataset(root='data/multinli_1.0_train.jsonl', \n",
    "                         type_map={'neutral':0, 'entailment':1, 'contradiction':2},\n",
    "                         tokenizer=BertTokenizerFast.from_pretrained('bert-base-uncased'),\n",
    "                         batch_size=bs,\n",
    "                         embedding_size=512\n",
    "                         )\n",
    "\n",
    "dev_data = MNLIDataset(root='data/multinli_1.0_dev_matched.jsonl', \n",
    "                       type_map={'neutral':0, 'entailment':1, 'contradiction':2},\n",
    "                       tokenizer=BertTokenizerFast.from_pretrained('bert-base-uncased'),\n",
    "                       batch_size=bs,\n",
    "                       embedding_size=512\n",
    "                       )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pass tokenization of sentences into bert and map the final embedding of the [CLS] token into 3D vector for 3-class classification\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, 3) # MNLI has 3 classes\n",
    "    \n",
    "    def forward(self, input):\n",
    "        x = self.bert(**input)\n",
    "        x = x.last_hidden_state[:, 0, :].squeeze() # pick out [CLS] final embedding\n",
    "        logits = self.fc(x)\n",
    "        return logits\n",
    "\n",
    "model = BertClassifier().to(device)\n",
    "model.load_state_dict(torch.load('data/bertmnli.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (6): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (7): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (8): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (9): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (10): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (11): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# look at model architecture\n",
    "print(model.bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does it work with initial layers frozen? Not really...\n",
    "for param in model.bert.embeddings.parameters():\n",
    "    param.detach_()\n",
    "frozen_layers = 8\n",
    "for layer in range(frozen_layers):\n",
    "    for param in model.bert.encoder.layer[layer].parameters():\n",
    "        param.detach_() # detaching all initial layers increases batch size on my single gpu by cutting off comp graph, saving memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.Adam(model.parameters(), lr = 2e-5)\n",
    "losses = []\n",
    "\n",
    "def train_epoch(model, loss_fn, optim, train_data, debug=False):\n",
    "    bs = train_data.bs\n",
    "    batch_num = train_data.get_batch_num()\n",
    "    for i in range(batch_num):\n",
    "        if debug:\n",
    "            print(f\"memory allocated before get_batch on round {i}: {torch.cuda.memory_allocated() // 1024 ** 2}\")\n",
    "            with profile(activities=[\n",
    "                ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True, profile_memory=True) as prof:\n",
    "                with record_function(\"model_inference\"):\n",
    "                    input, labels = train_data.get_batch(device)\n",
    "                    print(input['attention_mask'].shape)\n",
    "                    print(input['input_ids'][0:10])\n",
    "                    output = model(input) # why does output have larger memory footprint on full train data vs debug data?\n",
    "                    loss = loss_fn(output, labels)\n",
    "            print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "            prof.export_chrome_trace(\"trace2.json\")\n",
    "            print(f\"memory allocated after output on round {i}: {torch.cuda.memory_allocated() // 1024 ** 2}\")\n",
    "        \n",
    "        else:\n",
    "            input, labels = train_data.get_batch(device)\n",
    "            output = model(input)\n",
    "            loss = loss_fn(output, labels)\n",
    "\n",
    "            if i % 500 == 0:\n",
    "                losses.append(loss.item())\n",
    "                print(f\"loss={loss.item()} [{i * bs} / {batch_num * bs}]\")\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_num = 5\n",
    "for epochs in range(epoch_num):\n",
    "    train_epoch(model, loss_fn, optim, train_data)\n",
    "    print(f\"End of epoch {epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"bertmnli.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, loss_fn, data, losses):\n",
    "    with torch.no_grad():\n",
    "        batch_num = data.get_batch_num()\n",
    "        \n",
    "        for i in range(batch_num):\n",
    "            input, labels = data.get_batch(device)\n",
    "            preds = model(input)\n",
    "            loss = loss_fn(preds, labels)\n",
    "            losses.append(loss.item())\n",
    "            if i % 100 == 0:\n",
    "                print(f\"loss={loss.item()} [{i * bs} / {batch_num * bs}]\")\n",
    "            \n",
    "        print(f\"Avg loss: {sum(losses) / batch_num}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jarkr\\AppData\\Local\\Temp/ipykernel_21752/570165820.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data = {k:torch.tensor(self.data[self.idx][k], device=device) for k in self.data[self.idx]}\n",
      "C:\\Users\\jarkr\\AppData\\Local\\Temp/ipykernel_21752/570165820.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch_labels = torch.tensor(self.labels[self.idx*self.bs : (self.idx+1)*self.bs], device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=0.20467762649059296 [0 / 9808]\n",
      "loss=0.0006142045021988451 [800 / 9808]\n",
      "loss=1.7694125175476074 [1600 / 9808]\n",
      "loss=0.584679365158081 [2400 / 9808]\n",
      "loss=0.8866902589797974 [3200 / 9808]\n",
      "loss=1.0042858123779297 [4000 / 9808]\n",
      "loss=1.0168482065200806 [4800 / 9808]\n",
      "loss=1.634853720664978 [5600 / 9808]\n",
      "loss=0.054585762321949005 [6400 / 9808]\n",
      "loss=0.01108003593981266 [7200 / 9808]\n",
      "loss=1.797624945640564 [8000 / 9808]\n",
      "loss=0.3181334435939789 [8800 / 9808]\n",
      "loss=0.5450186133384705 [9600 / 9808]\n",
      "Avg loss: 0.7827006019944115\n"
     ]
    }
   ],
   "source": [
    "test(model, loss_fn, dev_data, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_num = 20\n",
    "for epochs in range(epoch_num):\n",
    "    train_epoch(model, loss_fn, optim, debug_data)\n",
    "    print(f\"End of epoch {epochs}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dffe948f5391847980490cb7274f2d250ba24d329a6f8b14b8642bdd49a9d95c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
