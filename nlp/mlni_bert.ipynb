{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import typing\n",
    "from transformers import BertTokenizer, BertTokenizerFast, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser as wb\n",
    "ref_list = [\n",
    "    \"https://huggingface.co/docs/transformers/model_doc/bert\",\n",
    "    \"https://huggingface.co/docs/transformers/v4.24.0/en/model_doc/bert#transformers.BertConfig\",\n",
    "    \"https://huggingface.co/docs/transformers/v4.24.0/en/model_doc/bert#transformers.BertTokenizer\",\n",
    "    \"https://arxiv.org/pdf/1810.04805.pdf\"\n",
    "]\n",
    "for url in ref_list:\n",
    "    wb.open(url)\n",
    "\n",
    "def subdict(d, keys):\n",
    "    return {key:d[key] for key in keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNLIDataset():\n",
    "    def __init__(self, root, type_map, tokenizer, batch_size, embedding_size):\n",
    "        self.idx = 0\n",
    "        self.type_map = type_map\n",
    "        self.bs = batch_size\n",
    "        self.es = embedding_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.df = pd.read_json(root, lines=True)\n",
    "        self.df = self.df[['sentence1', 'sentence2', 'gold_label']]\n",
    "        self.df['gold_label'] = self.df['gold_label'].apply(lambda x: self.type_mapper(x))\n",
    "\n",
    "    def type_mapper(self, text):\n",
    "        return self.type_map[text]    \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def set_idx(self, idx):\n",
    "        self.idx = idx\n",
    "\n",
    "    def get_batch(self):\n",
    "        # cycle through batches and restart if idx is too large\n",
    "        start = 0\n",
    "        if self.idx + self.bs < len(self):\n",
    "            start = self.idx\n",
    "        else:\n",
    "            self.idx = 0\n",
    "        end = min(start + self.bs, len(self))\n",
    "        data = self.tokenizer(self.df['sentence1'].iloc[start:end].tolist(), \n",
    "                              self.df['sentence2'].iloc[start:end].tolist(), \n",
    "                              return_tensors='pt',\n",
    "                              padding=True\n",
    "                              )\n",
    "        self.idx += self.bs\n",
    "        batch_input = subdict(data, ['input_ids', 'attention_mask', 'token_type_ids'])\n",
    "        batch_labels = torch.tensor(list(self.df['gold_label'].iloc[start:end]))\n",
    "        return batch_input, batch_labels \n",
    "        # return {'input_ids': torch.concat([ data['input_ids'], torch.zeros(self.bs, self.es - data['input_ids'].shape[1]) ], dim=1).to(dtype=torch.int)}\n",
    "\n",
    "train_data = MNLIDataset('mnli/multinli_1.0/train_debug.json', \n",
    "                         {'neutral':0, 'entailment':1, 'contradiction':2},\n",
    "                         BertTokenizerFast.from_pretrained('bert-base-uncased'),\n",
    "                         batch_size=32,\n",
    "                         embedding_size=512\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Use bert for classification by sending [cls] [tokenization of sentence1] [sep] [tokenization of sentence2]\n",
    "# into bert and then using embedding of [cls] as input to classifier\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, 3) # MNLI has 3 classes\n",
    "        \n",
    "    def forward(self, input):\n",
    "        x = self.bert(**input)\n",
    "        x = x.last_hidden_state[:, 0, :].squeeze() # pick out [CLS] final embedding\n",
    "        logits = self.fc(x)\n",
    "        return logits\n",
    "\n",
    "model = BertClassifier().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "frozen_layers = 10\n",
    "for layer in range(frozen_layers):\n",
    "    for param in model.bert.encoder.layer[layer].parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3])\n",
      "torch.Size([32])\n",
      "tensor([[ 0.4482, -0.2323,  0.2887],\n",
      "        [ 0.5486, -0.3704, -0.0165],\n",
      "        [ 0.3745, -0.1011,  0.1391],\n",
      "        [ 0.3059, -0.3736,  0.1182],\n",
      "        [ 0.5399, -0.3053,  0.0831],\n",
      "        [ 0.4903, -0.2889,  0.2261],\n",
      "        [ 0.6127, -0.2593,  0.0258],\n",
      "        [ 0.5393, -0.4015,  0.2033],\n",
      "        [ 0.2678, -0.2319, -0.0842],\n",
      "        [ 0.5263, -0.0207,  0.3270],\n",
      "        [ 0.2517,  0.0069,  0.1417],\n",
      "        [ 0.1881, -0.4228, -0.1700],\n",
      "        [ 0.4877, -0.5912, -0.0872],\n",
      "        [ 0.3928, -0.0749, -0.2020],\n",
      "        [ 0.5052, -0.3313,  0.1834],\n",
      "        [ 0.5916, -0.3129,  0.1199],\n",
      "        [ 0.3025, -0.2111, -0.0886],\n",
      "        [ 0.0874, -0.3144,  0.2670],\n",
      "        [ 0.4075, -0.3485,  0.1285],\n",
      "        [ 0.1913, -0.2660, -0.0771],\n",
      "        [ 0.3413, -0.1709, -0.1335],\n",
      "        [ 0.4636, -0.2988, -0.0686],\n",
      "        [ 0.2597, -0.1512, -0.0459],\n",
      "        [ 0.4030, -0.2846, -0.0413],\n",
      "        [ 0.5565, -0.2495, -0.0131],\n",
      "        [ 0.3969, -0.3045,  0.1001],\n",
      "        [ 0.3627, -0.4245,  0.1176],\n",
      "        [ 0.5188, -0.2928, -0.2341],\n",
      "        [ 0.3810, -0.1496,  0.0890],\n",
      "        [ 0.5267, -0.3428,  0.2152],\n",
      "        [ 0.5062, -0.3234,  0.0475],\n",
      "        [ 0.4692, -0.2535,  0.0054]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 1, 1, 1, 0, 1, 0, 1, 2, 2, 1, 2, 0, 0, 2, 1, 1, 2, 2, 0, 1, 1, 2, 1,\n",
      "        1, 0, 2, 0, 2, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "input, labels = train_data.get_batch()\n",
    "input, labels = {key:input[key].to(device) for key in input}, labels.to(device)\n",
    "output = model(input)\n",
    "print(f\"{output.shape}\\n{labels.shape}\")\n",
    "print(output)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor(1.1935, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "preds = torch.argmax(output, dim=1)\n",
    "print(preds)\n",
    "\n",
    "loss = loss_fn(output, labels)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(model.parameters())\n",
    "losses = []\n",
    "def train_epoch(model, loss_fn, optim, train_data):\n",
    "    bs = train_data.bs\n",
    "    train_example_num = len(train_data)\n",
    "    batch_num = train_example_num // bs if train_example_num % bs > 0 else train_example_num // bs + 1\n",
    "    \n",
    "    for i in range(batch_num):\n",
    "        input, labels = train_data.get_batch()\n",
    "        input, labels = {key:input[key].to(device) for key in input}, labels.to(device)\n",
    "        output = model(input)\n",
    "        \n",
    "        loss = loss_fn(output, labels)\n",
    "        if i % 2 == 0:\n",
    "            losses.append(loss.item())\n",
    "            print(f\"loss={loss.item()}.2f [{i * bs} / {train_example_num}]\")\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=1.124358057975769.2f [0 / 200]\n",
      "loss=2.788536310195923.2f [64 / 200]\n",
      "loss=1.2627285718917847.2f [128 / 200]\n",
      "End of epoch 0\n",
      "loss=0.8348004221916199.2f [0 / 200]\n",
      "loss=0.9568321704864502.2f [64 / 200]\n",
      "loss=0.7655072212219238.2f [128 / 200]\n",
      "End of epoch 1\n",
      "loss=0.27687278389930725.2f [0 / 200]\n",
      "loss=0.4760887324810028.2f [64 / 200]\n",
      "loss=0.195520281791687.2f [128 / 200]\n",
      "End of epoch 2\n",
      "loss=0.0427311547100544.2f [0 / 200]\n",
      "loss=0.30090799927711487.2f [64 / 200]\n",
      "loss=0.024307992309331894.2f [128 / 200]\n",
      "End of epoch 3\n",
      "loss=0.16933958232402802.2f [0 / 200]\n",
      "loss=0.30625542998313904.2f [64 / 200]\n",
      "loss=0.21990647912025452.2f [128 / 200]\n",
      "End of epoch 4\n",
      "loss=0.11982423067092896.2f [0 / 200]\n",
      "loss=0.045377954840660095.2f [64 / 200]\n",
      "loss=0.03413793444633484.2f [128 / 200]\n",
      "End of epoch 5\n",
      "loss=0.035563837736845016.2f [0 / 200]\n",
      "loss=0.010574637912213802.2f [64 / 200]\n",
      "loss=0.010204879567027092.2f [128 / 200]\n",
      "End of epoch 6\n",
      "loss=0.0032322166953235865.2f [0 / 200]\n",
      "loss=0.043013814836740494.2f [64 / 200]\n",
      "loss=0.0033315347973257303.2f [128 / 200]\n",
      "End of epoch 7\n",
      "loss=0.0033157854340970516.2f [0 / 200]\n",
      "loss=0.004081603139638901.2f [64 / 200]\n",
      "loss=0.018178552389144897.2f [128 / 200]\n",
      "End of epoch 8\n",
      "loss=0.0023079330567270517.2f [0 / 200]\n",
      "loss=0.004687610547989607.2f [64 / 200]\n",
      "loss=0.005351434461772442.2f [128 / 200]\n",
      "End of epoch 9\n",
      "loss=0.009824670851230621.2f [0 / 200]\n",
      "loss=0.0031552196014672518.2f [64 / 200]\n",
      "loss=0.0014409811701625586.2f [128 / 200]\n",
      "End of epoch 10\n",
      "loss=0.24096523225307465.2f [0 / 200]\n",
      "loss=0.0015496141277253628.2f [64 / 200]\n",
      "loss=0.001265683094970882.2f [128 / 200]\n",
      "End of epoch 11\n",
      "loss=0.0015281608793884516.2f [0 / 200]\n",
      "loss=0.004818985238671303.2f [64 / 200]\n",
      "loss=0.0022622861433774233.2f [128 / 200]\n",
      "End of epoch 12\n",
      "loss=0.00250455760397017.2f [0 / 200]\n",
      "loss=0.004531801212579012.2f [64 / 200]\n",
      "loss=0.001242580940015614.2f [128 / 200]\n",
      "End of epoch 13\n",
      "loss=0.0012278235517442226.2f [0 / 200]\n",
      "loss=0.001356138731352985.2f [64 / 200]\n",
      "loss=0.0007627730374224484.2f [128 / 200]\n",
      "End of epoch 14\n",
      "loss=0.0007093220483511686.2f [0 / 200]\n",
      "loss=0.0008288881508633494.2f [64 / 200]\n",
      "loss=0.0005670386599376798.2f [128 / 200]\n",
      "End of epoch 15\n",
      "loss=0.0005266431253403425.2f [0 / 200]\n",
      "loss=0.0006274859188124537.2f [64 / 200]\n",
      "loss=0.0004820294270757586.2f [128 / 200]\n",
      "End of epoch 16\n",
      "loss=0.0004354214761406183.2f [0 / 200]\n",
      "loss=0.0005334828165359795.2f [64 / 200]\n",
      "loss=0.0004248397599440068.2f [128 / 200]\n",
      "End of epoch 17\n",
      "loss=0.00037913559935986996.2f [0 / 200]\n",
      "loss=0.00047531124437227845.2f [64 / 200]\n",
      "loss=0.00038232552469708025.2f [128 / 200]\n",
      "End of epoch 18\n",
      "loss=0.0003396584070287645.2f [0 / 200]\n",
      "loss=0.00043215558980591595.2f [64 / 200]\n",
      "loss=0.00034882829640991986.2f [128 / 200]\n",
      "End of epoch 19\n"
     ]
    }
   ],
   "source": [
    "epoch_num = 20\n",
    "for epochs in range(epoch_num):\n",
    "    train_epoch(model, loss_fn, optim, train_data)\n",
    "    print(f\"End of epoch {epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's fitting!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ff702a5b878c217728eba8a5f5ee2c481ad215d8939aecc933a90145ccce5a54"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
